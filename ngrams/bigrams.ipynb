{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/delmirodaladiersampaioneto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package machado to\n",
      "[nltk_data]     /home/delmirodaladiersampaioneto/nltk_data...\n",
      "[nltk_data]   Package machado is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import machado\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('machado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and tokenizing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = machado.raw('romance/marm05.txt')\n",
    "test_text = machado.raw('romance/marm02.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_text.lower()\n",
    "test_text = test_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_text(train_text: str, file_name:str):\n",
    "\n",
    "    word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "\n",
    "    sentences = sent_tokenize(train_text, language='portuguese')\n",
    "\n",
    "    with open(file_name, 'w+') as file_writer:\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = word_tokenizer.tokenize(sentence.replace('\\n', ' '))\n",
    "            if tokenized_sentence:\n",
    "                tokenized_sentence.append('[END]')\n",
    "                tokenized_sentence = ['[START]'] + tokenized_sentence \n",
    "                tokenized_sentence = [token for token in tokenized_sentence if token != \"\\n\"]\n",
    "                file_writer.write(','.join(tokenized_sentence))\n",
    "                file_writer.write('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(filename):\n",
    "    n_gram_size = 2\n",
    "    sentences = []\n",
    "\n",
    "    counts = {}\n",
    "    prev_counts = {}\n",
    "    num_sentences = 0\n",
    "    num_tokens = 0\n",
    "    \n",
    "    \n",
    "    with open(filename, 'r') as op:\n",
    "        sentences = op.readlines()\n",
    "        sentences = [sentence.replace('\\n', '').split(',') for sentence in sentences]\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        num_sentences =+ 1 \n",
    "        num_tokens =+ len(sentence)\n",
    "        \n",
    "        #get count for bigrams\n",
    "        for index, ngram in enumerate(ngrams(sentence, n_gram_size)):\n",
    "            counts[ngram] = counts.get(ngram, 0) + 1\n",
    "            \n",
    "        # get count of unigrams\n",
    "        for index, ngram in enumerate(ngrams(sentence, n_gram_size - 1)): \n",
    "            prev_counts[ngram[0]] = prev_counts.get(ngram[0], 0) + 1\n",
    "            \n",
    "       \n",
    "    return counts, prev_counts, num_sentences, num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(counts, prev_counts):\n",
    "\n",
    "    probs = {}\n",
    "    vocabulary = counts.keys()\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    for bigram in counts.keys():\n",
    "        probs[bigram] = (counts[bigram] + 1)/(prev_counts[bigram[0]] + 1 * vocabulary_size)\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(train_bigram_counts, test_bigram_counts, probabilities, test_num_tokens):\n",
    "    \n",
    "    vocabulary = train_bigram_counts.keys()\n",
    "    test_log_likelihood = 0\n",
    "    \n",
    "    for bigram, bigram_count in test_bigram_counts.items():\n",
    "        if bigram not in vocabulary:\n",
    "            word = '[UNK]'\n",
    "            \n",
    "        train_prob = probabilities[bigram]\n",
    "        log_likelihood = bigram_count * np.log2(train_prob)\n",
    "        test_log_likelihood += log_likelihood\n",
    "\n",
    "        avg_test_log_likelihood = test_log_likelihood / test_num_tokens\n",
    "        \n",
    "        return avg_test_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenized_text(train_text, 'tokenized_train.txt')\n",
    "create_tokenized_text(test_text, 'tokenized_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, prev_counts, num_sentences, num_tokens = count_ngrams('tokenized_train.txt')\n",
    "test_counts, test_prev_counts, test_num_sentences, test_num_tokens = count_ngrams('tokenized_test.txt')\n",
    "probs = calculate_probabilities(counts, prev_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.811249475410533"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation(counts, test_counts, probs, test_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
