{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2QaluYfpnv09ywrFdSSPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DelmiroDaladier/NLP-studies/blob/master/text_summarizarion/text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wXRFAi0J2sw"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Ined9PZf2i",
        "outputId": "136e61df-76e3-4d2f-cb43-b69cfe60f261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcXMOovAL9PU"
      },
      "source": [
        "dataset = pd.read_csv('Reviews.csv', encoding=\"utf-8\", nrows=10000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOG8lbgLMCFo",
        "outputId": "29920ad0-adc9-405e-8bd6-7e046310d074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "dataset.sample(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9430</th>\n",
              "      <td>9431</td>\n",
              "      <td>B006N3IG4K</td>\n",
              "      <td>A154WMWOARJHLI</td>\n",
              "      <td>Gary Rosenfeld</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1284681600</td>\n",
              "      <td>Great Coffee</td>\n",
              "      <td>This is an excellent blend perfect for any tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>6470</td>\n",
              "      <td>B000FNEX50</td>\n",
              "      <td>A12SAU7NR89O98</td>\n",
              "      <td>A. Olson</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1350950400</td>\n",
              "      <td>Just Terrible.</td>\n",
              "      <td>We have a gluten free household of 5.  No one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6358</th>\n",
              "      <td>6359</td>\n",
              "      <td>B001AW9PTO</td>\n",
              "      <td>A33LQ9MA6PCUBK</td>\n",
              "      <td>W. Chiu</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1309219200</td>\n",
              "      <td>too salty</td>\n",
              "      <td>too salty -- enough said. I order the wood smo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1590</th>\n",
              "      <td>1591</td>\n",
              "      <td>B001CWZXIY</td>\n",
              "      <td>A2TPNALNIKN96P</td>\n",
              "      <td>Vanessa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1329091200</td>\n",
              "      <td>My dog loves this dog food.</td>\n",
              "      <td>My dog loves, loves  this dog food.I had to sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7775</th>\n",
              "      <td>7776</td>\n",
              "      <td>B000G7VYWU</td>\n",
              "      <td>ASRUYRC3R5J6Q</td>\n",
              "      <td>Betsy Heimbuch</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1309046400</td>\n",
              "      <td>IF YOU HAVE A PEANUT ALLERGY, BEWARE</td>\n",
              "      <td>My husband says these pretzels are great and h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id  ...                                               Text\n",
              "9430  9431  ...  This is an excellent blend perfect for any tim...\n",
              "6469  6470  ...  We have a gluten free household of 5.  No one ...\n",
              "6358  6359  ...  too salty -- enough said. I order the wood smo...\n",
              "1590  1591  ...  My dog loves, loves  this dog food.I had to sa...\n",
              "7775  7776  ...  My husband says these pretzels are great and h...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh_7DQAkj99v"
      },
      "source": [
        "dataset.drop_duplicates(subset ='Text', inplace=True)\n",
        "dataset.dropna(inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csrKvof7mkCy"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ6Z3I8krJI0"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    formatted_text = text.lower()\n",
        "    formatted_text = re.sub(r'\\([^)]*\\)', '', formatted_text)\n",
        "    formatted_text = re.sub('\"','', formatted_text)\n",
        "    formatted_text = re.sub(r'<.*?>', '', formatted_text)\n",
        "    formatted_Text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in formatted_text.split(\" \")])    \n",
        "    formatted_text = re.sub(r\"'s\\b\",\"\", formatted_text)\n",
        "    formatted_text = re.sub(\"[^a-zA-Z]\", \" \", formatted_text) \n",
        "    tokens = [word for word in formatted_text.split() if word not in stop_words]\n",
        "    \n",
        "\n",
        "    return (\" \".join(tokens).strip())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SfPQoRirkd7"
      },
      "source": [
        "dataset['cleaned_texts'] = dataset['Text'].apply(lambda x: clean_text(x))\n",
        "dataset['cleaned_summaries'] = dataset['Summary'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glgw6Mu3tshy"
      },
      "source": [
        "dataset['cleaned_texts'].replace('', np.nan, inplace=True)\n",
        "dataset['cleaned_texts'].dropna(axis=0, inplace=True)\n",
        "dataset['cleaned_summaries'].replace('', np.nan, inplace=True)\n",
        "dataset['cleaned_summaries'].dropna(axis=0, inplace=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfXBRcIkyDzs"
      },
      "source": [
        "dataset['cleaned_texts'] = dataset['cleaned_texts'].apply(lambda x : '<SOS> ' + str(x) + ' <EOS>')\n",
        "dataset['cleaned_summaries'] = dataset['cleaned_summaries'].apply(lambda x : '<SOS> ' + str(x) + ' <EOS>')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4QRQaMsfxsb"
      },
      "source": [
        "text_word_to_index = {}\n",
        "text_index_to_word = {}\n",
        "\n",
        "index = 0\n",
        "for text in dataset['cleaned_texts']:\n",
        "  for word in text.split():\n",
        "    if word not in text_word_to_index.keys():\n",
        "      text_word_to_index[word] = index\n",
        "      text_index_to_word[index] = word\n",
        "      index += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0uLZak_iiO4"
      },
      "source": [
        "text_vocab_size = len(text_word_to_index.keys())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmVMxF3isiB"
      },
      "source": [
        "summary_word_to_index = {}\n",
        "summary_index_to_word = {}\n",
        "\n",
        "index = 0\n",
        "for text in dataset['cleaned_summaries']:\n",
        "  for word in text.split():\n",
        "    if word not in summary_word_to_index.keys():\n",
        "      summary_word_to_index[word] = index\n",
        "      summary_index_to_word[index] = word\n",
        "      index += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhzPojT1xsES"
      },
      "source": [
        "summary_vocab_size = len(summary_word_to_index.keys())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO_Z4o3Lm3Yv"
      },
      "source": [
        "max_text_lenght = max([len(text.split()) for text in dataset['cleaned_texts']])\n",
        "max_summary_lenght = max([len(summary.split()) for summary in dataset['cleaned_summaries']]) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQv15MLnJIz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset['cleaned_texts'], dataset['cleaned_summaries'], test_size=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYLtdwnvtLQP"
      },
      "source": [
        "def batch_data_generator(x=X_train, y=y_train, batch_size=128):\n",
        "\n",
        "  while True:\n",
        "    for j in range(0, len(x), batch_size):\n",
        "      encoder_input_data = np.zeros((batch_size, max_text_lenght), dtype=\"float32\")\n",
        "      decoder_input_data = np.zeros((batch_size, max_summary_lenght), dtype=\"float32\")\n",
        "      decoder_target_data = np.zeros((batch_size, max_summary_lenght, summary_vocab_size), dtype=\"float32\")\n",
        "\n",
        "      for i, (input_text, target_summary) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
        "        for t, word in enumerate(input_text.split()):\n",
        "          encoder_input_data[i, t] = text_word_to_index[word]\n",
        "        for t, word in enumerate(target_summary.split()):\n",
        "          if t < len(target_summary.split())-1:\n",
        "            decoder_input_data[i, t] = summary_word_to_index[word]\n",
        "\n",
        "          if t > 0:\n",
        "            decoder_target_data[i, t - 1, summary_word_to_index[word]] = 1.0\n",
        "\n",
        "      yield([encoder_input_data, decoder_input_data], decoder_target_data) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgRXKRH8wMdA"
      },
      "source": [
        "K.clear_session()\n",
        "latent_dim = 128\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(text_vocab_size, latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(summary_vocab_size, latent_dim, mask_zero=True)\n",
        "dec_emb = decoder_embedding(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(\n",
        "    dec_emb,\n",
        "    initial_state=encoder_states)\n",
        "decoder_dense = Dense(summary_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKiz_1TuwkGp",
        "outputId": "fda06299-11e9-47e1-f929-fee7194df1bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 128)    2310656     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 128)    557952      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 128), (None, 131584      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  131584      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 4359)   562311      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,694,087\n",
            "Trainable params: 3,694,087\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl0L5B9M_WGh"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgGQCyygBFsK",
        "outputId": "e3b9a791-33ab-4931-f106-89aaa61c8f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_samples = len(X_train) # Total Training samples\n",
        "val_samples = len(X_test)    # Total validation or test samples\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "model.fit_generator(generator = batch_data_generator(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = batch_data_generator(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)\n",
        "\n",
        "model.save('text_summarization')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "66/66 [==============================] - 300s 5s/step - loss: 0.9422 - val_loss: 0.9111\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 284s 4s/step - loss: 0.8485 - val_loss: 0.9244\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 293s 4s/step - loss: 0.8363 - val_loss: 0.9369\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 279s 4s/step - loss: 0.8301 - val_loss: 0.9411\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 290s 4s/step - loss: 0.8218 - val_loss: 0.9469\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 282s 4s/step - loss: 0.8169 - val_loss: 0.9480\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 291s 4s/step - loss: 0.8048 - val_loss: 0.9504\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 293s 4s/step - loss: 0.7947 - val_loss: 0.9560\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 285s 4s/step - loss: 0.7827 - val_loss: 0.9525\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 281s 4s/step - loss: 0.7715 - val_loss: 0.9574\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 294s 4s/step - loss: 0.7580 - val_loss: 0.9561\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 281s 4s/step - loss: 0.7448 - val_loss: 0.9632\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 284s 4s/step - loss: 0.7295 - val_loss: 0.9590\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 281s 4s/step - loss: 0.7160 - val_loss: 0.9635\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 282s 4s/step - loss: 0.7004 - val_loss: 0.9694\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 280s 4s/step - loss: 0.6843 - val_loss: 0.9762\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 280s 4s/step - loss: 0.6696 - val_loss: 0.9865\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 285s 4s/step - loss: 0.6571 - val_loss: 0.9864\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 280s 4s/step - loss: 0.6398 - val_loss: 1.0026\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 285s 4s/step - loss: 0.6258 - val_loss: 1.0079\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 286s 4s/step - loss: 0.6139 - val_loss: 1.0015\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 294s 4s/step - loss: 0.5978 - val_loss: 1.0109\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 287s 4s/step - loss: 0.5839 - val_loss: 1.0198\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 288s 4s/step - loss: 0.5712 - val_loss: 1.0282\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 285s 4s/step - loss: 0.5570 - val_loss: 1.0436\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 289s 4s/step - loss: 0.5444 - val_loss: 1.0362\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 284s 4s/step - loss: 0.5314 - val_loss: 1.0425\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 291s 4s/step - loss: 0.5231 - val_loss: 1.0496\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 286s 4s/step - loss: 0.5082 - val_loss: 1.0564\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 289s 4s/step - loss: 0.4960 - val_loss: 1.0695\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 283s 4s/step - loss: 0.4841 - val_loss: 1.0799\n",
            "Epoch 32/100\n",
            "43/66 [==================>...........] - ETA: 1:35 - loss: 0.4686"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzRBAHleBkDL"
      },
      "source": [
        "model = load_model('text_summarization')\n",
        "\n",
        "encoder_inputs = model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_h_input = Input(shape=(latent_dim), name='hidden_state_input')\n",
        "decoder_state_c_input = Input(shape=(latent_dim), name='cell_state_input')\n",
        "decoder_states_inputs = [decoder_state_h_input, decoder_state_c_input]\n",
        "\n",
        "dec_emb_layer = model.layers[3]\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = model.layers[5]\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(\n",
        "    dec_emb, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [decoder_state_h, decoder_state_c]\n",
        "decoder_dense = model.layers[6]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w28wRoa0CiYW"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "  states_values = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq = np.zeros((1,1))\n",
        "\n",
        "  target_seq[0, 0] = summary_word_to_index['<SOS>']\n",
        "\n",
        "  decoded_sequence = ''\n",
        "  stop_condition = False\n",
        "  while not stop_condition:\n",
        "\n",
        "    output_tokens, h, c = decoder_model([target_seq] + states_values)\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_token = summary_index_to_word[sampled_token_index]\n",
        "\n",
        "    decoded_sequence += ' ' + sampled_token \n",
        "\n",
        "    if sampled_token == '<EOS>' or len(decoded_sequence.split()) > max_summary_lenght:\n",
        "            stop_condition = True\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "    states_value = [h, c]\n",
        "  \n",
        "  return decoded_sequence\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C9vh548DQDV",
        "outputId": "4d1f6f44-4f33-4329-cc61-99f40b4e7846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_test.reset_index(drop=True, inplace=True) \n",
        "y_test.reset_index(drop=True, inplace=True) \n",
        "test_gen = batch_data_generator(x=X_train, y=y_test, batch_size=1)\n",
        "for seq_index in range(10):\n",
        "  input_seq = next(test_gen)\n",
        "  decoded_summary = decode_sequence(input_seq)\n",
        "  print(\"-\")\n",
        "  print(\"Input sentence:\", X_test[seq_index])\n",
        "  print(\"Real sentence:\", y_test[seq_index])\n",
        "  print(\"Predicted sentence:\", decoded_summary)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: <SOS> really enjoy low calorie chips good flavor good crunch crisp wondering ordering case chips one could order favorites whole flavors say flavors desired great product stop manufacturing <EOS>\n",
            "Real sentence: <SOS> yummy yummies <EOS>\n",
            "Predicted sentence:  mill product <EOS>\n",
            "-\n",
            "Input sentence: <SOS> addicted gum tastes close real thing lose flavor compared apple pie strawberry best flavor series along mint chocolate chip <EOS>\n",
            "Real sentence: <SOS> delicious <EOS>\n",
            "Predicted sentence:  mill product <EOS>\n",
            "-\n",
            "Input sentence: <SOS> plugged direct windows controlled via ustream tv software interface worked well interim setup could get pro level interface one mic etc nice feels bit flimsy construction wise going take road <EOS>\n",
            "Real sentence: <SOS> used audience mic pickup <EOS>\n",
            "Predicted sentence:  mill coffee <EOS>\n",
            "-\n",
            "Input sentence: <SOS> brother owns keurig one cup coffee maker amazing device however drink coffee came across hot cocoa k cups took chance incredibly good gives chance share hot drink coffee crowd without boil water milk pack priced well coffee k cups well <EOS>\n",
            "Real sentence: <SOS> hot chocolate seconds <EOS>\n",
            "Predicted sentence:  mill product good product good product good product good product good product good product good product good product\n",
            "-\n",
            "Input sentence: <SOS> well drink surprisingly tasty reason surprising sugar added juice carbonated like drinking soda drink juice carbonated would good drink want healthy drink carbonated curious flavors see taste better however say expecting something extremely sweet drink get may want go buy kool aid drink natural taste actually taste mixture orange tangerine wish tangerine flavor stronger like tangerine orange considering fact sugar good drink <EOS>\n",
            "Real sentence: <SOS> surprisingly tasty <EOS>\n",
            "Predicted sentence:  mill coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee\n",
            "-\n",
            "Input sentence: <SOS> fifth different type coffee tried keurig coffee maker right tied newman k cup favorite couple k cup products seemed fine first tasted come pale somewhat comparison flavors makes really smooooooooooooooooooth tasting coffee strong taste goes easily richness taste well bottom line purchasing supply runs continue experimenting different flavor k cups <EOS>\n",
            "Real sentence: <SOS> oooh like <EOS>\n",
            "Predicted sentence:  mill <EOS>\n",
            "-\n",
            "Input sentence: <SOS> love stuff sweetens without affecting blood glucose levels could figure make tequila would set kidding <EOS>\n",
            "Real sentence: <SOS> agave syrup <EOS>\n",
            "Predicted sentence:  mill <EOS>\n",
            "-\n",
            "Input sentence: <SOS> best flavor box salt pepper bbq flavor flavor <EOS>\n",
            "Real sentence: <SOS> get salt pepper box instead <EOS>\n",
            "Predicted sentence:  mill product <EOS>\n",
            "-\n",
            "Input sentence: <SOS> favorite granola bar vegan gluten free difficult comes finding granola bars meet needs one absolutely delicious love ingredients strawberries throughout whole bar highly recommend <EOS>\n",
            "Real sentence: <SOS> favorite <EOS>\n",
            "Predicted sentence:  mill <EOS>\n",
            "-\n",
            "Input sentence: <SOS> earth best grourmet line heavy protein far know adequate protein important babies need like rest us good iron levels beef spinach contains grams per serving check cans use protein grams daughter likes also least expensive gourmet line something could find local stores recommended <EOS>\n",
            "Real sentence: <SOS> twice protein earth best baby food general <EOS>\n",
            "Predicted sentence:  mill dog product good product good product good product good product good product good product good product good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faMp9Fflphwy"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHU3P0kYqAhv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}